{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Transformer Architecture Exercise\n",
    "## Comparing GPT-2, BERT, and T5 on CNN/DailyMail Summarization\n",
    "\n",
    "**Student Name:** Rojesh Shrestha  \n",
    "**Course:** MSIT3103 - Generative AI  \n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This assignment compares three major transformer architectures on a text summarization task:\n",
    "- **GPT-2** (decoder-only)\n",
    "- **BERT** (encoder-only)  \n",
    "- **T5** (encoder-decoder)\n",
    "\n",
    "I chose the **CNN/DailyMail dataset** because it's a well-established benchmark for summarization tasks. Each article comes with a human-written summary, making it perfect for training and evaluating generative models. The dataset is large enough to be meaningful but I'll use a subset (500 training, 100 validation examples) to keep training time reasonable.\n",
    "\n",
    "### Why CNN/DailyMail?\n",
    "- It's specifically designed for summarization (unlike WikiText which is for language modeling)\n",
    "- Has clear input-output pairs (article → summary)\n",
    "- Lets us see how different architectures handle conditional generation\n",
    "- Widely used in research, so results are comparable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing required packages... This may take a few minutes.\n",
      "================================================================================\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "================================================================================\n",
      "Installation complete!\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All packages installed successfully!\n",
      "\n",
      "Package Versions:\n",
      "  - PyTorch: 2.10.0\n",
      "  - Transformers: 5.1.0\n",
      "  - Datasets: 4.5.0\n",
      "  - Pandas: 2.3.3\n",
      "\n",
      "Hardware:\n",
      "  - GPU Available: False\n",
      "  - Running on CPU (training will be slower)\n",
      "\n",
      "================================================================================\n",
      "Ready to proceed! You can now run the rest of the notebook.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Installation of required packages\n",
    "# This will take 2-5 minutes\n",
    "\n",
    "import sys\n",
    "\n",
    "# Install required packages\n",
    "print(\"Installing required packages... This may take a few minutes.\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "!{sys.executable} -m pip install -q torch transformers datasets evaluate rouge-score nltk pandas matplotlib accelerate\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Installation complete!\\n\")\n",
    "\n",
    "# Verify installation\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import datasets\n",
    "    import evaluate\n",
    "    import pandas\n",
    "    import matplotlib\n",
    "    import numpy\n",
    "    \n",
    "    print(\"✓ All packages installed successfully!\\n\")\n",
    "    print(\"Package Versions:\")\n",
    "    print(f\"  - PyTorch: {torch.__version__}\")\n",
    "    print(f\"  - Transformers: {transformers.__version__}\")\n",
    "    print(f\"  - Datasets: {datasets.__version__}\")\n",
    "    print(f\"  - Pandas: {pandas.__version__}\")\n",
    "    print(f\"\\nHardware:\")\n",
    "    print(f\"  - GPU Available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"  - GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"  - GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    else:\n",
    "        print(\"  - Running on CPU (training will be slower)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Ready to proceed! You can now run the rest of the notebook.\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n❌ Error: {e}\")\n",
    "    print(\"\\nPlease try installing manually:\")\n",
    "    print(\"  pip install torch transformers datasets evaluate rouge-score nltk pandas matplotlib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "================================================================================\n",
      "⚠️  MPS DISABLED - Training will run on CPU only\n",
      "This is necessary because my M2 Mac runs out of GPU memory.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import evaluate\n",
    "from transformers import logging\n",
    "\n",
    "# Silence warnings for cleaner output\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# COMPLETELY DISABLE MPS - FORCE CPU ONLY\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
    "torch.backends.mps.is_available = lambda: False\n",
    "torch.backends.mps.is_built = lambda: False\n",
    "\n",
    "# Force CPU device\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"⚠️  MPS DISABLED - Training will run on CPU only\")\n",
    "print(\"This is necessary because my M2 Mac runs out of GPU memory.\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dataset Preparation\n",
    "\n",
    "Loading CNN/DailyMail dataset and creating smaller subsets for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: dict_keys(['train', 'validation', 'test'])\n",
      "\n",
      "Training samples: 500\n",
      "Validation samples: 100\n",
      "\n",
      "================================================================================\n",
      "Sample Article (first 500 chars):\n",
      "By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Camborne, west Cornwall. The inquests have now opened into the deaths last Sat\n",
      "\n",
      "================================================================================\n",
      "Sample Summary:\n",
      "John and .\n",
      "Audrey Cook were discovered alongside their daughter, Maureen .\n",
      "They were found at Tremarle Home Park in Cornwall .\n",
      "Investigators say the three died of carbon monoxide .\n",
      "poisoning .\n"
     ]
    }
   ],
   "source": [
    "# Load the cnn_dailymail dataset (version 3.0.0)\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "# For quick experimentation, take a small subset\n",
    "train_size = 500\n",
    "val_size = 100\n",
    "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "small_val_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(val_size))\n",
    "\n",
    "print(\"Dataset splits:\", dataset.keys())\n",
    "print(f\"\\nTraining samples: {len(small_train_dataset)}\")\n",
    "print(f\"Validation samples: {len(small_val_dataset)}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Article (first 500 chars):\")\n",
    "print(small_train_dataset[0]['article'][:500])\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Summary:\")\n",
    "print(small_train_dataset[0]['highlights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Details\n",
    "\n",
    "**Tokenization:** Using model-specific tokenizers from HuggingFace\n",
    "- Max length: 512 tokens (to fit in memory)\n",
    "- Truncation: Yes (articles can be very long)\n",
    "- Padding: To max length for batch processing\n",
    "\n",
    "**Train/Val Split:** \n",
    "- Train: 500 examples\n",
    "- Validation: 100 examples\n",
    "- Random seed: 42 (for reproducibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Model 1 - GPT-2 (Decoder-Only)\n",
    "\n",
    "GPT-2 is a decoder-only transformer that generates text left-to-right. It's great at fluent text generation but can only see previous tokens (causal attention).\n",
    "\n",
    "### Training Approach:\n",
    "- Format: `\"summarize: [article]\"` → `[summary]`\n",
    "- The model learns to continue the prompt with a summary\n",
    "- We mask the prompt part in the loss (only train on summary tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: gpt2\n",
      "Vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model for GPT-2\n",
    "gpt2_model_name = \"gpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# GPT-2 doesn't have a padding token, so we set it to EOS\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded: {gpt2_model_name}\")\n",
    "print(f\"Vocab size: {len(gpt2_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input (first 100 tokens):\n",
      "summarize: By . Anthony Bond . PUBLISHED: . 07:03 EST, 2 March 2013 . | . UPDATED: . 08:07 EST, 2 March 2013 . Three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious 'within minutes', investigators said today. The bodies of married couple John and Audrey Cook were discovered alongside their daughter, Maureen, at the mobile home they shared on Tremarle Home Park in Cam\n"
     ]
    }
   ],
   "source": [
    "def preprocess_gpt2(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "    \n",
    "    # Tokenize separately to know where summary starts\n",
    "    model_inputs = gpt2_tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,  # Leave room for summary\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = gpt2_tokenizer(\n",
    "        targets,\n",
    "        max_length=112,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    # Combine: input + target\n",
    "    combined_input_ids = []\n",
    "    combined_labels = []\n",
    "    \n",
    "    for i in range(len(model_inputs[\"input_ids\"])):\n",
    "        input_ids = model_inputs[\"input_ids\"][i]\n",
    "        target_ids = labels[i]\n",
    "        \n",
    "        # Combine input and target\n",
    "        combined = input_ids + target_ids\n",
    "        if len(combined) > 512:\n",
    "            combined = combined[:512]\n",
    "        else:\n",
    "            combined = combined + [gpt2_tokenizer.pad_token_id] * (512 - len(combined))\n",
    "        \n",
    "        # Create labels: -100 for input, actual tokens for target\n",
    "        label_ids = [-100] * len(input_ids) + target_ids\n",
    "        if len(label_ids) > 512:\n",
    "            label_ids = label_ids[:512]\n",
    "        else:\n",
    "            label_ids = label_ids + [-100] * (512 - len(label_ids))\n",
    "        \n",
    "        combined_input_ids.append(combined)\n",
    "        combined_labels.append(label_ids)\n",
    "    \n",
    "    model_inputs[\"input_ids\"] = combined_input_ids\n",
    "    model_inputs[\"labels\"] = combined_labels\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_gpt2 = small_train_dataset.map(\n",
    "    preprocess_gpt2, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "val_gpt2 = small_val_dataset.map(\n",
    "    preprocess_gpt2, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Sample tokenized input (first 100 tokens):\")\n",
    "print(gpt2_tokenizer.decode(train_gpt2[0][\"input_ids\"][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to track losses\n",
    "class LossHistoryCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.steps = []\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if \"loss\" in logs:\n",
    "                self.train_losses.append(logs[\"loss\"])\n",
    "                self.steps.append(state.global_step)\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_losses.append(logs[\"eval_loss\"])\n",
    "\n",
    "# Initialize callback\n",
    "gpt2_callback = LossHistoryCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 148/148 [00:00<00:00, 1315.38it/s, Materializing param=transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: gpt2\n",
      "Parameters: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "# Data collator for causal LM\n",
    "data_collator_gpt2 = DataCollatorForLanguageModeling(\n",
    "    tokenizer=gpt2_tokenizer, \n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Load GPT-2 model\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "print(f\"Model loaded: {gpt2_model_name}\")\n",
    "print(f\"Parameters: {gpt2_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Batch size: 1\n",
      "  Learning rate: 5e-05\n",
      "  Epochs: 3\n",
      "  Gradient accumulation: 2\n",
      "  FP16: False\n"
     ]
    }
   ],
   "source": [
    "# Training arguments for GPT-2\n",
    "training_args_gpt2 = TrainingArguments(\n",
    "    output_dir=\"./gpt2-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=1, #Changed from 4 to 1 due to low memory\n",
    "    per_device_eval_batch_size=1, #Changed from 4 to 1 due to low memory\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {training_args_gpt2.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args_gpt2.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args_gpt2.num_train_epochs}\")\n",
    "print(f\"  Gradient accumulation: {training_args_gpt2.gradient_accumulation_steps}\")\n",
    "print(f\"  FP16: {training_args_gpt2.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callback to track training metrics\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.steps = []\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if 'loss' in logs:\n",
    "                self.train_losses.append(logs['loss'])\n",
    "                self.steps.append(state.global_step)\n",
    "            if 'eval_loss' in logs:\n",
    "                self.eval_losses.append(logs['eval_loss'])\n",
    "\n",
    "# Create callbacks for each model\n",
    "gpt2_callback = MetricsCallback()\n",
    "bert_callback = MetricsCallback()\n",
    "t5_callback = MetricsCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Trainer initialized. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer\n",
    "trainer_gpt2 = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=training_args_gpt2,\n",
    "    train_dataset=train_gpt2,\n",
    "    eval_dataset=val_gpt2,\n",
    "    data_collator=data_collator_gpt2,\n",
    "    callbacks=[gpt2_callback],\n",
    ")\n",
    "\n",
    "print(\"GPT-2 Trainer initialized. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPT-2 training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '3.429', 'grad_norm': '12.72', 'learning_rate': '2.4e-05', 'epoch': '0.1'}\n",
      "{'loss': '3.307', 'grad_norm': '9.538', 'learning_rate': '4.9e-05', 'epoch': '0.2'}\n",
      "{'eval_loss': '3.005', 'eval_runtime': '11.24', 'eval_samples_per_second': '8.898', 'eval_steps_per_second': '8.898', 'epoch': '0.2'}\n",
      "{'loss': '3.254', 'grad_norm': '11.99', 'learning_rate': '4.829e-05', 'epoch': '0.3'}\n",
      "{'loss': '3.211', 'grad_norm': '10.37', 'learning_rate': '4.65e-05', 'epoch': '0.4'}\n",
      "{'eval_loss': '3.014', 'eval_runtime': '11.26', 'eval_samples_per_second': '8.878', 'eval_steps_per_second': '8.878', 'epoch': '0.4'}\n",
      "{'loss': '3.242', 'grad_norm': '48.29', 'learning_rate': '4.471e-05', 'epoch': '0.5'}\n",
      "{'loss': '3.278', 'grad_norm': '16.03', 'learning_rate': '4.293e-05', 'epoch': '0.6'}\n",
      "{'eval_loss': '3.029', 'eval_runtime': '11.02', 'eval_samples_per_second': '9.071', 'eval_steps_per_second': '9.071', 'epoch': '0.6'}\n",
      "{'loss': '3.213', 'grad_norm': '17.04', 'learning_rate': '4.114e-05', 'epoch': '0.7'}\n",
      "{'loss': '3.326', 'grad_norm': '13.84', 'learning_rate': '3.936e-05', 'epoch': '0.8'}\n",
      "{'eval_loss': '3.038', 'eval_runtime': '11.08', 'eval_samples_per_second': '9.028', 'eval_steps_per_second': '9.028', 'epoch': '0.8'}\n",
      "{'loss': '3.236', 'grad_norm': '16.81', 'learning_rate': '3.757e-05', 'epoch': '0.9'}\n",
      "{'loss': '3.29', 'grad_norm': '21.77', 'learning_rate': '3.579e-05', 'epoch': '1'}\n",
      "{'eval_loss': '3.074', 'eval_runtime': '11.08', 'eval_samples_per_second': '9.029', 'eval_steps_per_second': '9.029', 'epoch': '1'}\n",
      "{'loss': '3.13', 'grad_norm': '22.04', 'learning_rate': '3.4e-05', 'epoch': '1.1'}\n",
      "{'loss': '3.182', 'grad_norm': '13.04', 'learning_rate': '3.221e-05', 'epoch': '1.2'}\n",
      "{'eval_loss': '3.033', 'eval_runtime': '11.09', 'eval_samples_per_second': '9.018', 'eval_steps_per_second': '9.018', 'epoch': '1.2'}\n",
      "{'loss': '3.138', 'grad_norm': '50.28', 'learning_rate': '3.043e-05', 'epoch': '1.3'}\n",
      "{'loss': '3.152', 'grad_norm': '22.27', 'learning_rate': '2.864e-05', 'epoch': '1.4'}\n",
      "{'eval_loss': '3.04', 'eval_runtime': '13.81', 'eval_samples_per_second': '7.241', 'eval_steps_per_second': '7.241', 'epoch': '1.4'}\n",
      "{'loss': '3.223', 'grad_norm': '26.9', 'learning_rate': '2.686e-05', 'epoch': '1.5'}\n",
      "{'loss': '3.104', 'grad_norm': '28.11', 'learning_rate': '2.507e-05', 'epoch': '1.6'}\n",
      "{'eval_loss': '3.044', 'eval_runtime': '13.38', 'eval_samples_per_second': '7.476', 'eval_steps_per_second': '7.476', 'epoch': '1.6'}\n",
      "{'loss': '3.169', 'grad_norm': '22.82', 'learning_rate': '2.329e-05', 'epoch': '1.7'}\n",
      "{'loss': '3.136', 'grad_norm': '22.08', 'learning_rate': '2.15e-05', 'epoch': '1.8'}\n",
      "{'eval_loss': '3.044', 'eval_runtime': '16.08', 'eval_samples_per_second': '6.219', 'eval_steps_per_second': '6.219', 'epoch': '1.8'}\n",
      "{'loss': '3.172', 'grad_norm': '19.05', 'learning_rate': '1.971e-05', 'epoch': '1.9'}\n",
      "{'loss': '3.147', 'grad_norm': '17.63', 'learning_rate': '1.793e-05', 'epoch': '2'}\n",
      "{'eval_loss': '3.043', 'eval_runtime': '12.75', 'eval_samples_per_second': '7.844', 'eval_steps_per_second': '7.844', 'epoch': '2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:02<00:00,  2.91s/it]\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '3.204', 'grad_norm': '56.32', 'learning_rate': '1.614e-05', 'epoch': '2.1'}\n",
      "{'loss': '3.134', 'grad_norm': '36.18', 'learning_rate': '1.436e-05', 'epoch': '2.2'}\n",
      "{'eval_loss': '3.048', 'eval_runtime': '14.71', 'eval_samples_per_second': '6.799', 'eval_steps_per_second': '6.799', 'epoch': '2.2'}\n",
      "{'loss': '3.146', 'grad_norm': '41.15', 'learning_rate': '1.257e-05', 'epoch': '2.3'}\n",
      "{'loss': '3.085', 'grad_norm': '24.58', 'learning_rate': '1.079e-05', 'epoch': '2.4'}\n",
      "{'eval_loss': '3.046', 'eval_runtime': '14.35', 'eval_samples_per_second': '6.971', 'eval_steps_per_second': '6.971', 'epoch': '2.4'}\n",
      "{'loss': '3.12', 'grad_norm': '19.15', 'learning_rate': '9e-06', 'epoch': '2.5'}\n",
      "{'loss': '3.13', 'grad_norm': '33.86', 'learning_rate': '7.214e-06', 'epoch': '2.6'}\n",
      "{'eval_loss': '3.049', 'eval_runtime': '14.11', 'eval_samples_per_second': '7.086', 'eval_steps_per_second': '7.086', 'epoch': '2.6'}\n",
      "{'loss': '3.235', 'grad_norm': '100.9', 'learning_rate': '5.429e-06', 'epoch': '2.7'}\n",
      "{'loss': '3.09', 'grad_norm': '38.4', 'learning_rate': '3.643e-06', 'epoch': '2.8'}\n",
      "{'eval_loss': '3.053', 'eval_runtime': '14.58', 'eval_samples_per_second': '6.857', 'eval_steps_per_second': '6.857', 'epoch': '2.8'}\n",
      "{'loss': '3.13', 'grad_norm': '23.16', 'learning_rate': '1.857e-06', 'epoch': '2.9'}\n",
      "{'loss': '3.105', 'grad_norm': '44.46', 'learning_rate': '7.143e-08', 'epoch': '3'}\n",
      "{'eval_loss': '3.055', 'eval_runtime': '13.93', 'eval_samples_per_second': '7.179', 'eval_steps_per_second': '7.179', 'epoch': '3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:02<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '910.9', 'train_samples_per_second': '1.647', 'train_steps_per_second': '0.823', 'train_loss': '3.191', 'epoch': '3'}\n",
      "================================================================================\n",
      "GPT-2 training completed!\n",
      "Final training loss: 3.1906\n"
     ]
    }
   ],
   "source": [
    "# Train GPT-2\n",
    "print(\"Starting GPT-2 training...\")\n",
    "print(\"=\"*80)\n",
    "gpt2_train_result = trainer_gpt2.train()\n",
    "print(\"=\"*80)\n",
    "print(\"GPT-2 training completed!\")\n",
    "print(f\"Final training loss: {gpt2_train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:02<00:00,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "trainer_gpt2.save_model(\"./gpt2-summarization-final\")\n",
    "print(\"GPT-2 model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model 2 - BERT (Encoder-Only)\n",
    "\n",
    "BERT is an encoder-only model designed for understanding tasks (classification, QA) not generation. It uses bidirectional attention and masked language modeling (MLM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: bert-base-uncased\n",
      "Vocab size: 30522\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "bert_model_name = \"bert-base-uncased\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "print(f\"Tokenizer loaded: {bert_model_name}\")\n",
    "print(f\"Vocab size: {len(bert_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokenized input (first 100 tokens):\n",
      "[CLS] by. anthony bond. published :. 07 : 03 est, 2 march 2013. |. updated :. 08 : 07 est, 2 march 2013. three members of the same family who died in a static caravan from carbon monoxide poisoning would have been unconscious ' within minutes ', investigators said today. the bodies of married couple john and audrey cook were discovered alongside their daughter, maureen, at the mobile home they shared on tremarle home park in camborne, west cornwall. the in\n"
     ]
    }
   ],
   "source": [
    "def preprocess_bert(examples):\n",
    "    concatenated_texts = [\n",
    "        art + \" [SEP] \" + summ \n",
    "        for art, summ in zip(examples[\"article\"], examples[\"highlights\"])\n",
    "    ]\n",
    "    \n",
    "    model_inputs = bert_tokenizer(\n",
    "        concatenated_texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_bert = small_train_dataset.map(\n",
    "    preprocess_bert, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "val_bert = small_val_dataset.map(\n",
    "    preprocess_bert, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Sample tokenized input (first 100 tokens):\")\n",
    "print(bert_tokenizer.decode(train_bert[0][\"input_ids\"][:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|█| 202/202 [00:00<00:00, 1530.46it/s, Materializing param=cls.predi"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: bert-base-uncased\n",
      "Parameters: 109,514,298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Data collator for MLM\n",
    "mlm_probability = 0.15\n",
    "data_collator_bert = DataCollatorForLanguageModeling(\n",
    "    tokenizer=bert_tokenizer, \n",
    "    mlm=True,\n",
    "    mlm_probability=mlm_probability\n",
    ")\n",
    "\n",
    "# Load BERT model\n",
    "bert_model = AutoModelForMaskedLM.from_pretrained(bert_model_name)\n",
    "print(f\"Model loaded: {bert_model_name}\")\n",
    "print(f\"Parameters: {bert_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  Batch size: 2\n",
      "  Learning rate: 5e-05\n",
      "  Epochs: 3\n",
      "  MLM probability: 0.15\n"
     ]
    }
   ],
   "source": [
    "# Initialize callback for BERT\n",
    "bert_callback = LossHistoryCallback()\n",
    "\n",
    "# Training arguments for BERT\n",
    "training_args_bert = TrainingArguments(\n",
    "    output_dir=\"./bert-mlm\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=2, #Reduced due to memory issue\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "  \n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {training_args_bert.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args_bert.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args_bert.num_train_epochs}\")\n",
    "print(f\"  MLM probability: {mlm_probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Trainer initialized. Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# Create Trainer\n",
    "trainer_bert = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args_bert,\n",
    "    train_dataset=train_bert,\n",
    "    eval_dataset=val_bert,\n",
    "    data_collator=data_collator_bert,\n",
    "    callbacks=[bert_callback],\n",
    ")\n",
    "\n",
    "print(\"BERT Trainer initialized. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT training...\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '2.509', 'grad_norm': '15.72', 'learning_rate': '2.4e-05', 'epoch': '0.1'}\n",
      "{'loss': '2.224', 'grad_norm': '11.84', 'learning_rate': '4.9e-05', 'epoch': '0.2'}\n",
      "{'eval_loss': '1.927', 'eval_runtime': '15.67', 'eval_samples_per_second': '6.381', 'eval_steps_per_second': '3.191', 'epoch': '0.2'}\n",
      "{'loss': '2.325', 'grad_norm': '11.48', 'learning_rate': '4.829e-05', 'epoch': '0.3'}\n",
      "{'loss': '2.208', 'grad_norm': '11.87', 'learning_rate': '4.65e-05', 'epoch': '0.4'}\n",
      "{'eval_loss': '1.92', 'eval_runtime': '15.98', 'eval_samples_per_second': '6.259', 'eval_steps_per_second': '3.13', 'epoch': '0.4'}\n",
      "{'loss': '2.101', 'grad_norm': '12.24', 'learning_rate': '4.471e-05', 'epoch': '0.5'}\n",
      "{'loss': '2.252', 'grad_norm': '9.957', 'learning_rate': '4.293e-05', 'epoch': '0.6'}\n",
      "{'eval_loss': '1.891', 'eval_runtime': '16.82', 'eval_samples_per_second': '5.944', 'eval_steps_per_second': '2.972', 'epoch': '0.6'}\n",
      "{'loss': '2.075', 'grad_norm': '13.34', 'learning_rate': '4.114e-05', 'epoch': '0.7'}\n",
      "{'loss': '2.108', 'grad_norm': '12.96', 'learning_rate': '3.936e-05', 'epoch': '0.8'}\n",
      "{'eval_loss': '1.865', 'eval_runtime': '17.66', 'eval_samples_per_second': '5.663', 'eval_steps_per_second': '2.832', 'epoch': '0.8'}\n",
      "{'loss': '2.218', 'grad_norm': '11.82', 'learning_rate': '3.757e-05', 'epoch': '0.9'}\n",
      "{'loss': '2.028', 'grad_norm': '12.76', 'learning_rate': '3.579e-05', 'epoch': '1'}\n",
      "{'eval_loss': '1.849', 'eval_runtime': '18.71', 'eval_samples_per_second': '5.344', 'eval_steps_per_second': '2.672', 'epoch': '1'}\n",
      "{'loss': '1.93', 'grad_norm': '10.07', 'learning_rate': '3.4e-05', 'epoch': '1.1'}\n",
      "{'loss': '2.027', 'grad_norm': '11.15', 'learning_rate': '3.221e-05', 'epoch': '1.2'}\n",
      "{'eval_loss': '1.762', 'eval_runtime': '17.02', 'eval_samples_per_second': '5.876', 'eval_steps_per_second': '2.938', 'epoch': '1.2'}\n",
      "{'loss': '1.964', 'grad_norm': '12.36', 'learning_rate': '3.043e-05', 'epoch': '1.3'}\n",
      "{'loss': '1.987', 'grad_norm': '12.49', 'learning_rate': '2.864e-05', 'epoch': '1.4'}\n",
      "{'eval_loss': '1.803', 'eval_runtime': '16.72', 'eval_samples_per_second': '5.982', 'eval_steps_per_second': '2.991', 'epoch': '1.4'}\n",
      "{'loss': '1.941', 'grad_norm': '15.11', 'learning_rate': '2.686e-05', 'epoch': '1.5'}\n",
      "{'loss': '1.923', 'grad_norm': '10.83', 'learning_rate': '2.507e-05', 'epoch': '1.6'}\n",
      "{'eval_loss': '1.826', 'eval_runtime': '16.76', 'eval_samples_per_second': '5.965', 'eval_steps_per_second': '2.983', 'epoch': '1.6'}\n",
      "{'loss': '1.965', 'grad_norm': '12.2', 'learning_rate': '2.329e-05', 'epoch': '1.7'}\n",
      "{'loss': '1.928', 'grad_norm': '11.22', 'learning_rate': '2.15e-05', 'epoch': '1.8'}\n",
      "{'eval_loss': '1.764', 'eval_runtime': '16.78', 'eval_samples_per_second': '5.959', 'eval_steps_per_second': '2.979', 'epoch': '1.8'}\n",
      "{'loss': '1.944', 'grad_norm': '9.985', 'learning_rate': '1.971e-05', 'epoch': '1.9'}\n",
      "{'loss': '1.736', 'grad_norm': '12.71', 'learning_rate': '1.793e-05', 'epoch': '2'}\n",
      "{'eval_loss': '1.791', 'eval_runtime': '16.85', 'eval_samples_per_second': '5.936', 'eval_steps_per_second': '2.968', 'epoch': '2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.35s/it]\n",
      "/opt/homebrew/lib/python3.10/site-packages/torch/utils/data/dataloader.py:775: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  super().__init__(loader)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': '1.888', 'grad_norm': '11.51', 'learning_rate': '1.614e-05', 'epoch': '2.1'}\n",
      "{'loss': '1.99', 'grad_norm': '12.11', 'learning_rate': '1.436e-05', 'epoch': '2.2'}\n",
      "{'eval_loss': '1.792', 'eval_runtime': '16.76', 'eval_samples_per_second': '5.968', 'eval_steps_per_second': '2.984', 'epoch': '2.2'}\n",
      "{'loss': '1.926', 'grad_norm': '9.997', 'learning_rate': '1.257e-05', 'epoch': '2.3'}\n",
      "{'loss': '1.691', 'grad_norm': '16.93', 'learning_rate': '1.079e-05', 'epoch': '2.4'}\n",
      "{'eval_loss': '1.705', 'eval_runtime': '16.4', 'eval_samples_per_second': '6.099', 'eval_steps_per_second': '3.049', 'epoch': '2.4'}\n",
      "{'loss': '1.827', 'grad_norm': '12.62', 'learning_rate': '9e-06', 'epoch': '2.5'}\n",
      "{'loss': '1.839', 'grad_norm': '12.73', 'learning_rate': '7.214e-06', 'epoch': '2.6'}\n",
      "{'eval_loss': '1.716', 'eval_runtime': '16.77', 'eval_samples_per_second': '5.965', 'eval_steps_per_second': '2.982', 'epoch': '2.6'}\n",
      "{'loss': '1.988', 'grad_norm': '11.62', 'learning_rate': '5.429e-06', 'epoch': '2.7'}\n",
      "{'loss': '1.821', 'grad_norm': '9.495', 'learning_rate': '3.643e-06', 'epoch': '2.8'}\n",
      "{'eval_loss': '1.745', 'eval_runtime': '16.73', 'eval_samples_per_second': '5.977', 'eval_steps_per_second': '2.988', 'epoch': '2.8'}\n",
      "{'loss': '1.878', 'grad_norm': '10.3', 'learning_rate': '1.857e-06', 'epoch': '2.9'}\n",
      "{'loss': '1.823', 'grad_norm': '9.718', 'learning_rate': '7.143e-08', 'epoch': '3'}\n",
      "{'eval_loss': '1.727', 'eval_runtime': '16.69', 'eval_samples_per_second': '5.99', 'eval_steps_per_second': '2.995', 'epoch': '3'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': '1669', 'train_samples_per_second': '0.899', 'train_steps_per_second': '0.449', 'train_loss': '2.002', 'epoch': '3'}\n",
      "================================================================================\n",
      "BERT training completed!\n",
      "Final training loss: 2.0022\n"
     ]
    }
   ],
   "source": [
    "# Train BERT\n",
    "bert_model = bert_model.to('cpu')\n",
    "print(\"Starting BERT training...\")\n",
    "print(\"=\"*80)\n",
    "bert_train_result = trainer_bert.train()\n",
    "print(\"=\"*80)\n",
    "print(\"BERT training completed!\")\n",
    "print(f\"Final training loss: {bert_train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|████████████████████████████████| 1/1 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Saving the model\n",
    "trainer_bert.save_model(\"./bert-mlm-final\")\n",
    "print(\"BERT model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Model 3 - T5 (Encoder-Decoder)\n",
    "\n",
    "T5 is an encoder-decoder model designed for text-to-text tasks. Perfect for summarization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load T5 tokenizer and model\n",
    "t5_model_name = \"t5-small\"\n",
    "t5_tokenizer = AutoTokenizer.from_pretrained(t5_model_name)\n",
    "\n",
    "print(f\"Tokenizer loaded: {t5_model_name}\")\n",
    "print(f\"Vocab size: {len(t5_tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_t5(examples):\n",
    "    prefix = \"summarize: \"\n",
    "    inputs = [prefix + art for art in examples[\"article\"]]\n",
    "    targets = examples[\"highlights\"]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = t5_tokenizer(\n",
    "        inputs,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    labels = t5_tokenizer(\n",
    "        targets,\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "train_t5 = small_train_dataset.map(\n",
    "    preprocess_t5, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "val_t5 = small_val_dataset.map(\n",
    "    preprocess_t5, \n",
    "    batched=True, \n",
    "    remove_columns=dataset[\"validation\"].column_names\n",
    ")\n",
    "\n",
    "print(\"Sample tokenized input:\")\n",
    "print(\"Input:\", t5_tokenizer.decode(train_t5[0][\"input_ids\"][:100]))\n",
    "print(\"\\nTarget:\", t5_tokenizer.decode(train_t5[0][\"labels\"][:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for seq2seq\n",
    "data_collator_t5 = DataCollatorForSeq2Seq(\n",
    "    tokenizer=t5_tokenizer,\n",
    "    model=t5_model_name\n",
    ")\n",
    "\n",
    "# Load T5 model\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(t5_model_name)\n",
    "print(f\"Model loaded: {t5_model_name}\")\n",
    "print(f\"Parameters: {t5_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize callback for T5\n",
    "t5_callback = LossHistoryCallback()\n",
    "\n",
    "# Training arguments for T5\n",
    "training_args_t5 = TrainingArguments(\n",
    "    output_dir=\"./t5-summarization\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=25,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {training_args_t5.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args_t5.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args_t5.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer\n",
    "trainer_t5 = Trainer(\n",
    "    model=t5_model,\n",
    "    args=training_args_t5,\n",
    "    train_dataset=train_t5,\n",
    "    eval_dataset=val_t5,\n",
    "    data_collator=data_collator_t5,\n",
    "    callbacks=[t5_callback],\n",
    ")\n",
    "\n",
    "print(\"T5 Trainer initialized. Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train T5\n",
    "print(\"Starting T5 training...\")\n",
    "print(\"=\"*80)\n",
    "t5_train_result = trainer_t5.train()\n",
    "print(\"=\"*80)\n",
    "print(\"T5 training completed!\")\n",
    "print(f\"Final training loss: {t5_train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer_t5.save_model(\"./t5-summarization-final\")\n",
    "print(\"T5 model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# GPT-2\n",
    "if len(gpt2_callback.train_losses) > 0:\n",
    "    axes[0].plot(gpt2_callback.steps, gpt2_callback.train_losses, label='Training Loss', marker='o', markersize=3)\n",
    "    axes[0].set_title('GPT-2 Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Steps')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# BERT\n",
    "if len(bert_callback.train_losses) > 0:\n",
    "    axes[1].plot(bert_callback.steps, bert_callback.train_losses, label='Training Loss', marker='o', markersize=3, color='orange')\n",
    "    axes[1].set_title('BERT Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Steps')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# T5\n",
    "if len(t5_callback.train_losses) > 0:\n",
    "    axes[2].plot(t5_callback.steps, t5_callback.train_losses, label='Training Loss', marker='o', markersize=3, color='green')\n",
    "    axes[2].set_title('T5 Training Loss', fontsize=14, fontweight='bold')\n",
    "    axes[2].set_xlabel('Steps')\n",
    "    axes[2].set_ylabel('Loss')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/home/claude/training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training curves saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ROUGE metric\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "# Prepare test samples\n",
    "test_samples = small_val_dataset.select(range(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summaries_gpt2(model, tokenizer, articles, max_length=100):\n",
    "    summaries = []\n",
    "    model.eval()\n",
    "    \n",
    "    for article in articles:\n",
    "        prompt = f\"summarize: {article}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )\n",
    "        \n",
    "        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summary = full_text[len(prompt):].strip()\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def generate_summaries_t5(model, tokenizer, articles, max_length=128):\n",
    "    summaries = []\n",
    "    model.eval()\n",
    "    \n",
    "    for article in articles:\n",
    "        prompt = f\"summarize: {article}\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=max_length,\n",
    "                num_beams=4,\n",
    "                early_stopping=True,\n",
    "                no_repeat_ngram_size=2,\n",
    "            )\n",
    "        \n",
    "        summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    return summaries\n",
    "\n",
    "def extract_summary_bert(text, num_sentences=3):\n",
    "    sentences = text.split('. ')\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return text[:200]\n",
    "    summary = '. '.join(sentences[:num_sentences]) + '.'\n",
    "    return summary\n",
    "\n",
    "def generate_summaries_bert(model, tokenizer, articles):\n",
    "    summaries = []\n",
    "    for article in articles:\n",
    "        summary = extract_summary_bert(article)\n",
    "        summaries.append(summary)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move models to device\n",
    "gpt2_model.to(device)\n",
    "bert_model.to(device)\n",
    "t5_model.to(device)\n",
    "\n",
    "print(\"Generating summaries with GPT-2...\")\n",
    "gpt2_summaries = generate_summaries_gpt2(gpt2_model, gpt2_tokenizer, test_samples[\"article\"])\n",
    "\n",
    "print(\"Generating summaries with T5...\")\n",
    "t5_summaries = generate_summaries_t5(t5_model, t5_tokenizer, test_samples[\"article\"])\n",
    "\n",
    "print(\"Extracting summaries with BERT...\")\n",
    "bert_summaries = generate_summaries_bert(bert_model, bert_tokenizer, test_samples[\"article\"])\n",
    "\n",
    "# Reference summaries\n",
    "references = test_samples[\"highlights\"]\n",
    "\n",
    "print(\"\\nAll summaries generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "gpt2_rouge = rouge.compute(predictions=gpt2_summaries, references=references, use_stemmer=True)\n",
    "t5_rouge = rouge.compute(predictions=t5_summaries, references=references, use_stemmer=True)\n",
    "bert_rouge = rouge.compute(predictions=bert_summaries, references=references, use_stemmer=True)\n",
    "\n",
    "print(\"ROUGE Scores Calculated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in a table\n",
    "import pandas as pd\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['GPT-2', 'BERT', 'T5'],\n",
    "    'ROUGE-1': [\n",
    "        f\"{gpt2_rouge['rouge1']:.4f}\",\n",
    "        f\"{bert_rouge['rouge1']:.4f}\",\n",
    "        f\"{t5_rouge['rouge1']:.4f}\"\n",
    "    ],\n",
    "    'ROUGE-2': [\n",
    "        f\"{gpt2_rouge['rouge2']:.4f}\",\n",
    "        f\"{bert_rouge['rouge2']:.4f}\",\n",
    "        f\"{t5_rouge['rouge2']:.4f}\"\n",
    "    ],\n",
    "    'ROUGE-L': [\n",
    "        f\"{gpt2_rouge['rougeL']:.4f}\",\n",
    "        f\"{bert_rouge['rougeL']:.4f}\",\n",
    "        f\"{t5_rouge['rougeL']:.4f}\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('/home/claude/evaluation_results.csv', index=False)\n",
    "print(\"\\nResults saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample outputs\n",
    "sample_idx = 0\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE OUTPUT COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nOriginal Article (first 300 chars):\\n{test_samples[sample_idx]['article'][:300]}...\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nReference Summary:\\n{references[sample_idx]}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nGPT-2 Summary:\\n{gpt2_summaries[sample_idx]}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nBERT Summary (Extractive):\\n{bert_summaries[sample_idx]}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nT5 Summary:\\n{t5_summaries[sample_idx]}\\n\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
